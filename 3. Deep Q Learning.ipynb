{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "967be5a9-789f-46da-8a7c-ffb446a5f7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output\n",
    "k = tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f04b878-b083-4632-834c-fc99ac8728b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "k.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c9e7309-afc6-496d-a762-bea3ae23fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qmodel = k.Sequential([\n",
    "    k.layers.Input(shape=[4]),\n",
    "    k.layers.Dense(units=8,activation='relu'),\n",
    "    k.layers.Dense(units=8,activation='relu'),\n",
    "    k.layers.Dense(units=8,activation='relu'),\n",
    "    k.layers.Dense(units=2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8a56ce-7f22-48ab-85fe-3e0522a4dbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "QtargetModel = k.models.clone_model(Qmodel)\n",
    "QtargetModel.set_weights(Qmodel.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2c0a47-fa94-4b42-8e66-4b98f0d9fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qmodel.compile('adam',loss=k.losses.huber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e8c8f3-a630-48a4-a1d0-c8bee90d534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bc5415-e75d-4dd0-91c3-cf41064fa9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_action(state):\n",
    "    Qsa = Qmodel(state)[0].numpy()\n",
    "    return np.argmax(Qsa)\n",
    "\n",
    "def best_Q_value(state):\n",
    "    Qsa = QtargetModel(state)[0].numpy()\n",
    "    return Qsa[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f60bd4-0d7e-42a2-aae0-d633211e9dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1\n",
    "MIN_EPSILON = 0.05\n",
    "DECAY = 0.005\n",
    "DISCOUNT = 0.9\n",
    "\n",
    "MAX_HISTORY = 1000\n",
    "replay_memory = []\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6138af60-9615-40e2-b121-1d7766cd3337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    memories = random.choices(replay_memory, k=BATCH_SIZE)\n",
    "    batch = []\n",
    "    target = []\n",
    "    for sample in memories:\n",
    "        tmp = Qmodel(sample['state'])[0].numpy()\n",
    "        if sample['done']:\n",
    "            tmp[sample['action']] = sample['reward']\n",
    "            target.append(tmp)\n",
    "        else:\n",
    "            tmp[sample['action']] = sample['reward'] + DISCOUNT * best_Q_value(sample['next_state'])\n",
    "            target.append(tmp)\n",
    "            \n",
    "        batch.append(sample['state'][0])\n",
    "    \n",
    "    target = np.array(target)\n",
    "    batch = np.array(batch)\n",
    "    h = Qmodel.fit(batch, target, verbose=0)\n",
    "    \n",
    "    return h.history['loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4173d615-9e6e-40fa-9d19-5d3c8946e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    'rewards':[],\n",
    "    'losses':[],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe6212-b8df-42db-960a-e99beaa6cc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_GAME = 5000\n",
    "game = 0\n",
    "step = 0\n",
    "MAX_ITER = 200\n",
    "good_game = 0\n",
    "while game < MAX_GAME:\n",
    "    game += 1\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = np.array([state])\n",
    "    \n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "    for i in range(MAX_ITER):\n",
    "        if np.random.uniform() < EPSILON:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = best_action(state)\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = np.array([next_state])\n",
    "        \n",
    "        cumulative_reward += reward\n",
    "        \n",
    "        replay_memory.append({\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'next_state': next_state,\n",
    "            'done': done,\n",
    "        })\n",
    "        \n",
    "        if len(replay_memory) > MAX_HISTORY:\n",
    "            replay_memory.pop(0)\n",
    "            \n",
    "        step += 1\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    if EPSILON > MIN_EPSILON:\n",
    "        EPSILON = EPSILON * np.exp(-DECAY)\n",
    "        \n",
    "    loss = train()\n",
    "    if game % 50 == 0:\n",
    "        QtargetModel.set_weights(Qmodel.get_weights())\n",
    "        \n",
    "    \n",
    "    history['rewards'].append(cumulative_reward)\n",
    "    history['losses'].append(loss)\n",
    "    \n",
    "    print(f'Game {game}: {cumulative_reward} --- loss = {loss}')\n",
    "    \n",
    "    if cumulative_reward > 170:\n",
    "        good_game+=1\n",
    "    else:\n",
    "        good_game=0\n",
    "        \n",
    "    if good_game >= 100:\n",
    "        break\n",
    "    \n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8637476-58d8-43b9-a775-0f5af9562865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec7b850-b7e2-4f61-916f-68702bff3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['rewards'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad6b067-98c4-45fc-bc7e-734dc61a5b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['losses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6331241-1435-44af-bb9a-c2f49691124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state = np.array([state])\n",
    "done = False\n",
    "while not done:\n",
    "    action = best_action(state)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    next_state = np.array([next_state])\n",
    "    env.render()\n",
    "    \n",
    "    state = next_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6112d1d-cc17-428b-8976-d6e28b6d32bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('myenv': conda)",
   "language": "python",
   "name": "python381064bitmyenvcondac5ec00a7585646e8ae89d74ab82e4cd8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
